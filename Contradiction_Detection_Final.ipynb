{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "#nlp = spacy.load('en')\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explacy\n",
    "\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "_do_print_debug_info = False\n",
    "\n",
    "def _print_table(rows):\n",
    "    col_widths = [max(len(s) for s in col) for col in zip(*rows)]\n",
    "    fmt = ' '.join('%%-%ds' % width for width in col_widths)\n",
    "    rows.insert(1, ['─' * width for width in col_widths])\n",
    "    for row in rows:\n",
    "        # Uncomment this version to see code points printed out (for debugging).\n",
    "        # print(list(map(hex, map(ord, list(fmt % tuple(row))))))\n",
    "        print(fmt % tuple(row))\n",
    "\n",
    "def _start_end(arrow):\n",
    "    start, end = arrow['from'].i, arrow['to'].i\n",
    "    mn = min(start, end)\n",
    "    mx = max(start, end)\n",
    "    return start, end, mn, mx\n",
    "\n",
    "def print_parse_info(nlp, sent):\n",
    "    \"\"\" Print the dependency tree of `sent` (sentence), along with the lemmas\n",
    "        (de-inflected forms) and parts-of-speech of the words.\n",
    "\n",
    "        The input `sent` is expected to be a unicode string (of type unicode in\n",
    "        Python 2; of type str in Python 3). The input `nlp` (for natural\n",
    "        language parser) is expected to be the return value from a call to\n",
    "        spacy.load(), in other words, it's the callable instance of a spacy\n",
    "        language model.\n",
    "    \"\"\"\n",
    "\n",
    "    unicode_type = unicode if sys.version_info[0] < 3 else str\n",
    "    assert type(sent) is unicode_type\n",
    "\n",
    "    # Parse our sentence.\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Build a list of arrow heights (distance from tokens) per token.\n",
    "    heights = [[] for token in doc]\n",
    "\n",
    "    # Build the arrows.\n",
    "\n",
    "    # Set the from and to tokens for each arrow.\n",
    "    arrows = [{'from': src, 'to': dst, 'underset': set()}\n",
    "              for src in doc\n",
    "              for dst in src.children]\n",
    "\n",
    "    # Set the base height; these may increase to allow room for arrowheads after this.\n",
    "    arrows_with_deps = defaultdict(set)\n",
    "    for i, arrow in enumerate(arrows):\n",
    "        if _do_print_debug_info:\n",
    "            print('Arrow %d: \"%s\" -> \"%s\"' % (i, arrow['from'], arrow['to']))\n",
    "        num_deps = 0\n",
    "        start, end, mn, mx = _start_end(arrow)\n",
    "        for j, other in enumerate(arrows):\n",
    "            if arrow is other:\n",
    "                continue\n",
    "            o_start, o_end, o_mn, o_mx = _start_end(other)\n",
    "            if ((start == o_start and mn <= o_end <= mx) or\n",
    "                (start != o_start and mn <= o_start <= mx)):\n",
    "                num_deps += 1\n",
    "                if _do_print_debug_info:\n",
    "                    print('%d is over %d' % (i, j))\n",
    "                arrow['underset'].add(j)\n",
    "        arrow['num_deps_left'] = arrow['num_deps'] = num_deps\n",
    "        arrows_with_deps[num_deps].add(i)\n",
    "\n",
    "    if _do_print_debug_info:\n",
    "        print('')\n",
    "        print('arrows:')\n",
    "        pprint(arrows)\n",
    "\n",
    "        print('')\n",
    "        print('arrows_with_deps:')\n",
    "        pprint(arrows_with_deps)\n",
    "\n",
    "    # Render the arrows in characters. Some heights will be raised to make room for arrowheads.\n",
    "\n",
    "    lines = [[] for token in doc]\n",
    "    num_arrows_left = len(arrows)\n",
    "    while num_arrows_left > 0:\n",
    "\n",
    "        assert len(arrows_with_deps[0])\n",
    "\n",
    "        arrow_index = arrows_with_deps[0].pop()\n",
    "        arrow = arrows[arrow_index]\n",
    "        src, dst, mn, mx = _start_end(arrow)\n",
    "\n",
    "        # Check the height needed.\n",
    "        height = 3\n",
    "        if arrow['underset']:\n",
    "            height = max(arrows[i]['height'] for i in arrow['underset']) + 1\n",
    "        height = max(height, 3, len(lines[dst]) + 3)\n",
    "        arrow['height'] = height\n",
    "\n",
    "        if _do_print_debug_info:\n",
    "            print('')\n",
    "            print('Rendering arrow %d: \"%s\" -> \"%s\"' % (arrow_index,\n",
    "                                                        arrow['from'],\n",
    "                                                        arrow['to']))\n",
    "            print('  height = %d' % height)\n",
    "\n",
    "        goes_up = src > dst\n",
    "\n",
    "        # Draw the outgoing src line.\n",
    "        if lines[src] and len(lines[src]) < height:\n",
    "            lines[src][-1].add('w')\n",
    "        while len(lines[src]) < height - 1:\n",
    "            lines[src].append(set(['e', 'w']))\n",
    "        if len(lines[src]) < height:\n",
    "            lines[src].append({'e'})\n",
    "        lines[src][height - 1].add('n' if goes_up else 's')\n",
    "\n",
    "        # Draw the incoming dst line.\n",
    "        lines[dst].append(u'►')\n",
    "        while len(lines[dst]) < height:\n",
    "            lines[dst].append(set(['e', 'w']))\n",
    "        lines[dst][-1] = set(['e', 's']) if goes_up else set(['e', 'n'])\n",
    "\n",
    "        # Draw the adjoining vertical line.\n",
    "        for i in range(mn + 1, mx):\n",
    "            while len(lines[i]) < height - 1:\n",
    "                lines[i].append(' ')\n",
    "            lines[i].append(set(['n', 's']))\n",
    "\n",
    "        # Update arrows_with_deps.\n",
    "        for arr_i, arr in enumerate(arrows):\n",
    "            if arrow_index in arr['underset']:\n",
    "                arrows_with_deps[arr['num_deps_left']].remove(arr_i)\n",
    "                arr['num_deps_left'] -= 1\n",
    "                arrows_with_deps[arr['num_deps_left']].add(arr_i)\n",
    "\n",
    "        num_arrows_left -= 1\n",
    "\n",
    "    arr_chars = {'ew'  : u'─',\n",
    "                 'ns'  : u'│',\n",
    "                 'en'  : u'└',\n",
    "                 'es'  : u'┌',\n",
    "                 'enw' : u'┴',\n",
    "                 'ensw': u'┼',\n",
    "                 'esw' : u'┬'}\n",
    "\n",
    "    # Convert the character lists into strings.\n",
    "    max_len = max(len(line) for line in lines)\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = [arr_chars[''.join(sorted(ch))] if type(ch) is set else ch for ch in lines[i]]\n",
    "        lines[i] = ''.join(reversed(lines[i]))\n",
    "        lines[i] = ' ' * (max_len - len(lines[i])) + lines[i]\n",
    "\n",
    "    # Compile full table to print out.\n",
    "    rows = [['Dep tree', 'Token', 'Dep type', 'Lemma', 'Part of Sp']]\n",
    "    for i, token in enumerate(doc):\n",
    "        rows.append([lines[i], token, token.dep_, token.lemma_, token.pos_])\n",
    "    _print_table(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antonyms finder function\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "#word=\"went\"\n",
    "def antysyn(word):\n",
    "    from nltk.corpus import wordnet\n",
    "    for syn in wordnet.synsets(word):\n",
    "        #print(syn)\n",
    "        for l in syn.lemmas():\n",
    "            #print(l)\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    #print(\"Synonym:\",set(synonyms))\n",
    "    print(\"Antonym:\",set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree Token Dep type Lemma  Part of Sp\n",
      "──────── ───── ──────── ────── ──────────\n",
      "    ┌─►  I     nsubj    -PRON- PRON      \n",
      "┌┬──┴──  slept ROOT     sleep  VERB      \n",
      "│└─►┌──  till  prep     till   ADP       \n",
      "│   └─►  noon  pobj     noon   NOUN      \n",
      "└─────►  .     punct    .      PUNCT     \n",
      "\n",
      "\n",
      "Dep tree   Token   Dep type Lemma   Part of Sp\n",
      "────────── ─────── ──────── ─────── ──────────\n",
      "       ┌─► I       nsubj    -PRON-  PRON      \n",
      "┌┬────┬┼── woke    ROOT     wake    VERB      \n",
      "││    │└─► up      prt      up      PART      \n",
      "││    └──► early   advmod   early   ADV       \n",
      "│└─►┌───── in      prep     in      ADP       \n",
      "│   │  ┌─► the     det      the     DET       \n",
      "│   └─►└── morning pobj     morning NOUN      \n",
      "└────────► .       punct    .       PUNCT     \n"
     ]
    }
   ],
   "source": [
    "#Taking the sentences as input\n",
    "#sent1=input(\"Input the first sentence:\")\n",
    "#sent2=input(\"Input the second sentence:\")\n",
    "nlp = en_core_web_sm.load()\n",
    "sent1=\"I slept till noon.\"\n",
    "sent2=\"I woke up early in the morning.\"\n",
    "print_parse_info(nlp,sent1)\n",
    "print(\"\\n\")\n",
    "print_parse_info(nlp,sent2)\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing required variables and lists.\n",
    "wrdlist=list()\n",
    "antony=list()\n",
    "contr_tracker=0\n",
    "antonym_tracker=0\n",
    "\n",
    "negdoc1=0\n",
    "negdoc2=0\n",
    "verb1=\"\"\n",
    "verb2=\"\"\n",
    "num_contr_tracker=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slept\n",
      "Antonym: {'wake'}\n"
     ]
    }
   ],
   "source": [
    "#Processing sentence 1\n",
    "for token in doc1:\n",
    "    if(token.dep_==\"neg\"):\n",
    "        negdoc1=1\n",
    "        verb1+=\"NOT \"\n",
    "    #Storing the antonyms of root words\n",
    "    if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
    "        print(token.text)\n",
    "        verb1+=token.lemma_\n",
    "        antysyn(token.lemma_)\n",
    "        for anton in antonyms:\n",
    "            antony.append(anton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Sentence 2\n",
    "for token in doc2:\n",
    "    if(token.dep_==\"neg\"):\n",
    "        negdoc2=1\n",
    "        verb2+=\"NOT \"\n",
    "    if(token.pos_==\"VERB\" and token.dep_==\"ROOT\"):\n",
    "        verb2+=token.lemma_\n",
    "        if(token.lemma_ in antony):\n",
    "            antonym_tracker=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for checking negation\n",
    "def checknegationcontradiction(antonym_tracker,negdoc1,negdoc2):\n",
    "    temp_var=negdoc1+negdoc2+antonym_tracker\n",
    "    if(temp_var%2!=0 and temp_var<3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking contradiction due to negation\n",
    "contr_tracker=checknegationcontradiction(antonym_tracker,negdoc1,negdoc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding numerical mismatch\n",
    "checklist_more=['more than ', 'greater than ', 'above']\n",
    "checklist_less=['less than ', 'lesser than ', 'below']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_words(doc):\n",
    "    merged_word=\"\"\n",
    "    #tokens = word_tokenize(doc)\n",
    "    WordNum  = len(doc)\n",
    "    print(\"WordNum is:\"+str(WordNum))\n",
    "    for i in range(WordNum):\n",
    "        print(i,doc[i],doc[i].ent_type_)\n",
    "        print(str.isdigit(doc[i].text))\n",
    "        if(doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\"):\n",
    "             merged_word = doc[i].text\n",
    "        if((doc[i].ent_type_==\"CARDINAL\" or doc[i].pos_==\"NUM\") and str.isdigit(doc[i].text)):\n",
    "            if(not(str.isdigit(doc[i-1].text))):\n",
    "                if(not(str.isdigit(doc[i-2].text))):\n",
    "                    merged_word= doc[i-2].text+' '+doc[i-1].text+' '+doc[i].text\n",
    "    return merged_word    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(t1,t2):\n",
    "    for phrase in checklist_more:\n",
    "        if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
    "            num1 = t1.replace(phrase,'')\n",
    "            num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
    "            num2=num2[0]\n",
    "            if int(num1)>num2:\n",
    "                print(\"case1\")\n",
    "                return('Contradiction')\n",
    "            else:\n",
    "                return(\"No Contradiction\")\n",
    "        else:\n",
    "            return(\"No Contradiction\")\n",
    "    for phrase in checklist_more:\n",
    "        if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
    "            num2 = t2.replace(phrase,'')\n",
    "            num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
    "            num1=num1[0]\n",
    "            if int(num2)>num1:\n",
    "                print(\"case2\")\n",
    "                return('Contradiction') \n",
    "            else:\n",
    "                return(\"No Contradiction\")\n",
    "        else:\n",
    "            return(\"No Contradiction\")\n",
    "    for phrase in checklist_less:\n",
    "        if(t1.find(phrase)!=-1 and t2.find(phrase)==-1):\n",
    "            num1 = t1.replace(phrase,'')\n",
    "            num2 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
    "            num2=num2[0]\n",
    "            if int(num1)<num2:\n",
    "                print(\"case3\")\n",
    "                return('Contradiction')\n",
    "            else:\n",
    "                return(\"No Contradiction\")\n",
    "        else:\n",
    "                return(\"No Contradiction\")\n",
    "    for phrase in checklist_less:\n",
    "        if(t2.find(phrase)!=-1 and t1.find(phrase)==-1):\n",
    "            num2 = t2.replace(phrase,'')\n",
    "            num1 = [int(s) for s in str.split(t2) if s.isdigit()]\n",
    "            num1=num1[0]\n",
    "            if int(num1)>num2:\n",
    "                print(\"case4\")\n",
    "                return('Contradiction')\n",
    "            else: \n",
    "                return(\"No Contradiction\")\n",
    "        else:\n",
    "                return(\"No Contradiction\")\n",
    "    else:\n",
    "        if(t1!=t2):\n",
    "            return('Contradiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNum is:5\n",
      "0 I \n",
      "False\n",
      "1 slept \n",
      "False\n",
      "2 till \n",
      "False\n",
      "3 noon TIME\n",
      "False\n",
      "4 . \n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = check_words(doc1)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNum is:8\n",
      "0 I \n",
      "False\n",
      "1 woke \n",
      "False\n",
      "2 up \n",
      "False\n",
      "3 early TIME\n",
      "False\n",
      "4 in TIME\n",
      "False\n",
      "5 the TIME\n",
      "False\n",
      "6 morning TIME\n",
      "False\n",
      "7 . \n",
      "False\n",
      "Second Sentence: \n"
     ]
    }
   ],
   "source": [
    "y1 = check_words(doc2)\n",
    "print(\"Second Sentence:\",y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_contr_tracker=check_values(x1,y1)\n",
    "if(number_contr_tracker=='Contradiction'):\n",
    "    num_contr_tracker=1\n",
    "else:\n",
    "    num_contr_tracker=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> SLEEP and WAKE can't happen simultaneously.\n",
      "->Antonymity/Negation contradiction FOUND.\n",
      "->Numeric Mismatch Contradiction NOT Found.\n"
     ]
    }
   ],
   "source": [
    "if contr_tracker==1:\n",
    "    print(\"\\n\",\"->\",verb1.upper(),\"and\",verb2.upper(),\"can't happen simultaneously.\")\n",
    "    print(\"->Antonymity/Negation contradiction FOUND.\")\n",
    "else:\n",
    "    print(\"\\n->Antonymity/Negation contradiction NOT found.\")\n",
    "\n",
    "if num_contr_tracker==1:\n",
    "    print(\"->Numeric Mismatch Contradiction FOUND.\")\n",
    "else:\n",
    "    print(\"->Numeric Mismatch Contradiction NOT Found.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
